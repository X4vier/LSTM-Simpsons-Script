{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TV Script Generation\n",
    "In this notebook, I train a recurrent neural network with long-short-term-memory (LSTM) cells to automatically generate tv scripts for the  [Simpsons](https://en.wikipedia.org/wiki/The_Simpsons). This project was completed in January 2018 as part of Udacityâ€™s deep learning nanodegree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "The data we use for this project is the list of Simpson's script lines provided by Kaggle. It can be downloaded [here](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data). Let's have a look at our data first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_text\n",
      "Miss Hoover: No, actually, it was a little of both. Sometimes when a disease is in all the magazines and all the news shows, it's only natural that you think you have it.\n",
      "Lisa Simpson: (NEAR TEARS) Where's Mr. Bergstrom?\n",
      "Miss Hoover: I don't know. Although I'd sure like to talk to him. He didn't touch my lesson plan. What did he teach you?\n",
      "Lisa Simpson: That life is worth living.\n",
      "Edna Krabappel-Flanders: The polls will be open from now until the end of recess. Now, (SOUR) just in case any of you have decided to put any thought into this, we'll have our final statements. Martin?\n",
      "Martin Prince: (HOARSE WHISPER) I don't think there's anything left to say.\n",
      "Edna Krabappel-Flanders: Bart?\n",
      "Bart Simpson: Victory party under the slide!\n",
      "\n",
      "(Apartment Building: Ext. apartment building - day)\n",
      "Lisa Simpson: (CALLING) Mr. Bergstrom! Mr. Bergstrom!\n",
      "Landlady: Hey, hey, he Moved out this morning. He must have a new job -- he took his Copernicus costume.\n",
      "Lisa Simpson: Do you know where I could find him?\n",
      "Landlady: I think he's taking the next train to Capital City.\n",
      "Lisa Simpson: The train, how like him... traditional, yet environmentally sound.\n",
      "Landlady: Yes, and it's been the backbone of our country since Leland Stanford drove that golden spike at Promontory point.\n",
      "Lisa Simpson: I see he touched you, too.\n",
      "\n",
      "(Springfield Elementary School: EXT. ELEMENTARY - SCHOOL PLAYGROUND - AFTERNOON)\n",
      "Bart Simpson: Hey, thanks for your vote, man.\n",
      "Nelson Muntz: I didn't vote. Voting's for geeks.\n",
      "Bart Simpson: Well, you got that right. (TO TERRI AND SHERRI) Thanks for your vote, girls.\n",
      "Terri/sherri: We forgot.\n",
      "Bart Simpson: Well, don't sweat it. Just so long as a couple of people did... right, Milhouse?\n",
      "Milhouse Van Houten: Uh oh.\n",
      "Bart Simpson: Lewis?\n",
      "Bart Simpson: (LOUDER) Somebody must have voted.\n",
      "Milhouse Van Houten: What about you, Bart? Didn't you vote?\n",
      "Bart Simpson: Uh oh.\n",
      "Bart Simpson: (ANGUISHED SCREAM)\n",
      "Wendell Borton: Yayyyyyyyyyyyyyy!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Location of data file\n",
    "data_dir = './data/simpsons/simpsons_script_lines.csv'\n",
    "\n",
    "# Print out some lines of the script\n",
    "display_lines = 30\n",
    "\n",
    "csvfile = open(data_dir, 'rt', encoding=\"utf8\")\n",
    "reader = csv.reader(csvfile)\n",
    "\n",
    "for row in reader:\n",
    "    line = row[3]\n",
    "    if line[0] == '(':\n",
    "        print()\n",
    "    print(line)\n",
    "    display_lines -= 1\n",
    "    if display_lines <= 0:\n",
    "        break\n",
    "        \n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data\n",
    "We preprocess the data by converting each word in the dataset into a distinct integer (this representation will be useful for when the model learns its own word2vec embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup Table\n",
    "The `create_lookup_tables()` function creates:\n",
    "- A dictionary to go from the words to an id, we'll call `vocab_to_int`\n",
    "- A dictionary to go from the id to word, we'll call `int_to_vocab`\n",
    "\n",
    "And returns them as the tuple `(vocab_to_int, int_to_vocab)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    vocab = set(text)\n",
    "    vocab_to_int = {word: idx for idx, word in enumerate(vocab)}\n",
    "    int_to_vocab = dict(enumerate(vocab))\n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Punctuation\n",
    "We'll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks make it hard for the neural network to distinguish between the word \"bye\" and \"bye!\".\n",
    "\n",
    "Implement the function `token_lookup` to return a dict that will be used to tokenize symbols like \"!\" into \"||Exclamation_Mark||\".  Create a dictionary for the following symbols where the symbol is the key and value is the token:\n",
    "- Period ( . )\n",
    "- Comma ( , )\n",
    "- Quotation Mark ( \" )\n",
    "- Semicolon ( ; )\n",
    "- Exclamation mark ( ! )\n",
    "- Question mark ( ? )\n",
    "- Left Parentheses ( ( )\n",
    "- Right Parentheses ( ) )\n",
    "- Dash ( -- )\n",
    "- Return ( \\n )\n",
    "\n",
    "This dictionary will be used to token the symbols and add the delimiter (space) around it.  This separates the symbols as it's own word, making it easier for the neural network to predict on the next word. Make sure you don't use a token that could be confused as a word. Instead of using the token \"dash\", try using something like \"||dash||\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    return {\n",
    "        '.': \"||Period||\",\n",
    "        ',': \"||Comma||\",\n",
    "        '\"': \"||Quotation_Mark||\",\n",
    "        ';': \"||Semicolon||\",\n",
    "        '!': \"||Exclamation_Mark||\",\n",
    "        '?': \"||Question_Mark||\",\n",
    "        '(': \"||Left_Parentheses||\",\n",
    "        ')': \"||Right_Parentheses||\",\n",
    "        '--': \"||Dash||\",\n",
    "        '\\n': \"||Return||\"\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the data and save it to file. Since I'm on my laptop right now (which doesn't have a fast GPU) we only import the first 30,000 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 30000 lines.\r"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Create array for storing all the data\n",
    "text = []\n",
    "\n",
    "current_line = 1\n",
    "total_num_lines = 30000\n",
    "\n",
    "token_dict = token_lookup()\n",
    "    \n",
    "\n",
    "with open(data_dir, 'rt', encoding=\"utf8\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        if current_line%100 == 0:\n",
    "            print(\"Processed {} lines.\".format(current_line), end=\"\\r\")\n",
    "        current_line += 1\n",
    "        \n",
    "        line = row[3]\n",
    "        # Replace punctuation with tokens\n",
    "        for key, token in token_dict.items():\n",
    "            line = line.replace(key, ' {} '.format(token))\n",
    "        \n",
    "        # Convert to lower case\n",
    "        line = line.lower()\n",
    "        line = line.split()\n",
    "        \n",
    "        if line[0] == token_dict['('].lower():\n",
    "            text.append([token_dict['\\n'].lower()])\n",
    "\n",
    "        text.append(line + [token_dict['\\n'].lower()])\n",
    "\n",
    "        if current_line > total_num_lines:\n",
    "            break\n",
    "\n",
    "# Concatenate into single list of words and tokens\n",
    "text = np.concatenate(text)\n",
    "\n",
    "# Remove junk at the start of file\n",
    "text = text[2:]\n",
    "\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(text)\n",
    "int_text = [vocab_to_int[word] for word in text]\n",
    "\n",
    "# Save processed data for later\n",
    "pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "Now lets reload the data we just saved and make sure we can recover the text back from the list of integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss hoover: no, actually, it was a little of both. sometimes when a disease is in all the magazines and all the news shows, it's only natural that you think you have it.\n",
      "lisa simpson:(near tears) where's mr. bergstrom?\n",
      "miss hoover: i don't know. although i'd sure like to talk to him. he didn't touch my lesson plan. what did he teach you?\n",
      "lisa simpson: that life is worth living.\n",
      "edna krabappel-flanders: the polls will be open from now until the end of recess. now,(sour) just in case any of you have decided to put any thought into this, we'll have our final statements. martin?\n",
      "martin prince:(hoarse whisper) i don't think there's anything left to say.\n",
      "edna krabappel-flanders: bart?\n",
      "bart simpson: victory party under the slide!\n",
      "\n",
      "(apartment building: ext. apartment building - day)\n",
      "lisa simpson:(calling) mr. bergstrom! mr. bergstrom!\n",
      "landlady: hey, hey, he moved out this morning.\n"
     ]
    }
   ],
   "source": [
    "recovered_text = \"\"\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = pickle.load(open('preprocess.p', 'rb'))\n",
    "\n",
    "for i in range (200):\n",
    "    word = int_to_vocab[int_text[i]]\n",
    "    pre = \" \"\n",
    "    # Undo tokenisation\n",
    "    for key, token in token_dict.items():\n",
    "        if word == token.lower():\n",
    "            word = key\n",
    "            pre = \"\"\n",
    "    recovered_text = recovered_text + pre + word\n",
    "\n",
    "# Remove unwanted spaces after punctuation\n",
    "recovered_text = recovered_text.replace('\\n ', '\\n')\n",
    "recovered_text = recovered_text.replace('( ', '(')\n",
    "\n",
    "# Remove space at the beggining\n",
    "recovered_text = recovered_text[1:]\n",
    "            \n",
    "\n",
    "print(recovered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name = \"input\")\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name = \"targets\")\n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "    return inputs, targets, learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN Cell and Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "\n",
    "    lstm_layers = 2 #TODO: make passable parameter\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(rnn_size) for _ in range(lstm_layers)])\n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = tf.identity(cell.zero_state(batch_size, tf.float32), name = \"initial_state\")\n",
    "\n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    # TODO: Experiment with different initialization\n",
    "    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    Outputs, FinalState = tf.nn.dynamic_rnn(cell, inputs, dtype = tf.float32)\n",
    "    return Outputs, tf.identity(FinalState, name = \"final_state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network\n",
    "Apply the functions you implemented above to:\n",
    "- Apply embedding to `input_data` using your `get_embed(input_data, vocab_size, embed_dim)` function.\n",
    "- Build RNN using `cell` and your `build_rnn(cell, inputs)` function.\n",
    "- Apply a fully connected layer with a linear activation and `vocab_size` as the number of outputs.\n",
    "\n",
    "Return the logits and final state in the following tuple (Logits, FinalState) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    embed = get_embed(input_data, vocab_size, embed_dim)\n",
    "    rnn_outputs, final_state = build_rnn(cell, embed)\n",
    "    logits = tf.contrib.layers.fully_connected(inputs = rnn_outputs, num_outputs = vocab_size, activation_fn=None)\n",
    "    \n",
    "    return logits, final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch the Data\n",
    "\n",
    "Note that since we are trying to predict the next word, the targets are the labels shifted by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    characters_per_batch = batch_size * seq_length\n",
    "    num_batches = len(int_text) // characters_per_batch\n",
    "    int_text = int_text[:characters_per_batch * num_batches]\n",
    "    \n",
    "    # Create offset target array\n",
    "    int_text_targets = np.zeros_like(int_text)\n",
    "    int_text_targets[:-1], int_text_targets[-1] = int_text[1:], int_text[0]\n",
    "    \n",
    "    # Reshape into batch_size rows\n",
    "    int_text = np.array(int_text).reshape([batch_size, -1])\n",
    "    int_text_targets = np.array(int_text_targets).reshape([batch_size, -1])\n",
    "    \n",
    "    batches = []\n",
    "    \n",
    "    for i in range(0, int_text.shape[1], seq_length):\n",
    "        x = np.array(int_text[:, i:i+seq_length])\n",
    "        y = np.array(int_text_targets[:, i:i+seq_length])\n",
    "        batches.append([x, y]) \n",
    "    return np.array(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 80\n",
    "# Batch Size\n",
    "batch_size = 100\n",
    "# RNN Size\n",
    "rnn_size = 256\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 256\n",
    "# Sequence Length\n",
    "seq_length = 100\n",
    "# Learning Rate\n",
    "learning_rate = 0.01\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 1\n",
    "\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Train the neural network on the preprocessed data.  If you have a hard time getting a good loss, check the [forums](https://discussions.udacity.com/) to see if anyone is having the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 48/49, train_loss = 6.489\n",
      "Epoch 1, Batch 48/49, train_loss = 6.342\n",
      "Epoch 2, Batch 48/49, train_loss = 6.214\n",
      "Epoch 3, Batch 48/49, train_loss = 6.082\n",
      "Epoch 4, Batch 48/49, train_loss = 5.949\n",
      "Epoch 5, Batch 48/49, train_loss = 5.686\n",
      "Epoch 6, Batch 48/49, train_loss = 5.508\n",
      "Epoch 7, Batch 48/49, train_loss = 5.363\n",
      "Epoch 8, Batch 48/49, train_loss = 5.221\n",
      "Epoch 9, Batch 48/49, train_loss = 5.048\n",
      "Epoch 10, Batch 48/49, train_loss = 4.855\n",
      "Epoch 11, Batch 48/49, train_loss = 4.681\n",
      "Epoch 12, Batch 48/49, train_loss = 4.510\n",
      "Epoch 13, Batch 48/49, train_loss = 4.369\n",
      "Epoch 14, Batch 48/49, train_loss = 4.284\n",
      "Epoch 15, Batch 48/49, train_loss = 4.147\n",
      "Epoch 16, Batch 48/49, train_loss = 4.041\n",
      "Epoch 17, Batch 48/49, train_loss = 3.961\n",
      "Epoch 18, Batch 48/49, train_loss = 3.884\n",
      "Epoch 19, Batch 48/49, train_loss = 3.826\n",
      "Epoch 20, Batch 48/49, train_loss = 3.788\n",
      "Epoch 21, Batch 48/49, train_loss = 3.708\n",
      "Epoch 22, Batch 48/49, train_loss = 3.643\n",
      "Epoch 23, Batch 48/49, train_loss = 3.584\n",
      "Epoch 24, Batch 48/49, train_loss = 3.526\n",
      "Epoch 25, Batch 48/49, train_loss = 3.466\n",
      "Epoch 26, Batch 48/49, train_loss = 3.408\n",
      "Epoch 27, Batch 48/49, train_loss = 3.356\n",
      "Epoch 28, Batch 48/49, train_loss = 3.281\n",
      "Epoch 29, Batch 48/49, train_loss = 3.237\n",
      "Epoch 30, Batch 48/49, train_loss = 3.188\n",
      "Epoch 31, Batch 48/49, train_loss = 3.145\n",
      "Epoch 32, Batch 48/49, train_loss = 3.092\n",
      "Epoch 33, Batch 48/49, train_loss = 3.065\n",
      "Epoch 34, Batch 48/49, train_loss = 3.020\n",
      "Epoch 35, Batch 48/49, train_loss = 2.969\n",
      "Epoch 36, Batch 48/49, train_loss = 2.923\n",
      "Epoch 37, Batch 48/49, train_loss = 2.893\n",
      "Epoch 38, Batch 48/49, train_loss = 2.865\n",
      "Epoch 39, Batch 48/49, train_loss = 2.848\n",
      "Epoch 40, Batch 48/49, train_loss = 2.807\n",
      "Epoch 41, Batch 48/49, train_loss = 2.758\n",
      "Epoch 42, Batch 48/49, train_loss = 2.726\n",
      "Epoch 43, Batch 48/49, train_loss = 2.698\n",
      "Epoch 44, Batch 48/49, train_loss = 2.670\n",
      "Epoch 45, Batch 48/49, train_loss = 2.633\n",
      "Epoch 46, Batch 48/49, train_loss = 2.610\n",
      "Epoch 47, Batch 48/49, train_loss = 2.570\n",
      "Epoch 48, Batch 48/49, train_loss = 2.536\n",
      "Epoch 49, Batch 48/49, train_loss = 2.501\n",
      "Epoch 50, Batch 48/49, train_loss = 2.464\n",
      "Epoch 51, Batch 48/49, train_loss = 2.450\n",
      "Epoch 52, Batch 48/49, train_loss = 2.416\n",
      "Epoch 53, Batch 48/49, train_loss = 2.379\n",
      "Epoch 54, Batch 48/49, train_loss = 2.338\n",
      "Epoch 55, Batch 48/49, train_loss = 2.324\n",
      "Epoch 56, Batch 48/49, train_loss = 2.294\n",
      "Epoch 57, Batch 48/49, train_loss = 2.280\n",
      "Epoch 58, Batch 48/49, train_loss = 2.266\n",
      "Epoch 59, Batch 48/49, train_loss = 2.244\n",
      "Epoch 60, Batch 48/49, train_loss = 2.232\n",
      "Epoch 61, Batch 48/49, train_loss = 2.225\n",
      "Epoch 62, Batch 48/49, train_loss = 2.203\n",
      "Epoch 63, Batch 48/49, train_loss = 2.184\n",
      "Epoch 64, Batch 48/49, train_loss = 2.176\n",
      "Epoch 65, Batch 48/49, train_loss = 2.157\n",
      "Epoch 66, Batch 48/49, train_loss = 2.137\n",
      "Epoch 67, Batch 48/49, train_loss = 2.129\n",
      "Epoch 68, Batch 48/49, train_loss = 2.109\n",
      "Epoch 69, Batch 48/49, train_loss = 2.101\n",
      "Epoch 70, Batch 48/49, train_loss = 2.075\n",
      "Epoch 71, Batch 48/49, train_loss = 2.077\n",
      "Epoch 72, Batch 48/49, train_loss = 2.064\n",
      "Epoch 73, Batch 48/49, train_loss = 2.048\n",
      "Epoch 74, Batch 48/49, train_loss = 2.036\n",
      "Epoch 75, Batch 48/49, train_loss = 2.023\n",
      "Epoch 76, Batch 48/49, train_loss = 2.009\n",
      "Epoch 77, Batch 48/49, train_loss = 1.989\n",
      "Epoch 78, Batch 48/49, train_loss = 1.976\n",
      "Epoch 79, Batch 48/49, train_loss = 1.970\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {}, Batch {}/{}, train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss), end=\"\\r\")\n",
    "        print()\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Parameters\n",
    "Save `seq_length` and `save_dir` for generating a new TV script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save parameters for checkpoint\n",
    "pickle.dump((seq_length, save_dir), open('params.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = pickle.load(open(\"preprocess.p\", mode='rb'))\n",
    "seq_length, load_dir = pickle.load(open('params.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    InputTensor = loaded_graph.get_tensor_by_name(name = \"input:0\")\n",
    "    InitialStateTensor = loaded_graph.get_tensor_by_name(\"initial_state:0\")\n",
    "    FinalStateTensor = loaded_graph.get_tensor_by_name(\"final_state:0\")\n",
    "    ProbsTensor = loaded_graph.get_tensor_by_name(\"probs:0\")\n",
    "    \n",
    "    return InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Word\n",
    "The `pick_word()` function samples words according to distribution specified in the `probabilities` vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    probs_vec = np.squeeze(probabilities)\n",
    "    r = random.random()\n",
    "    word_idx = -1\n",
    "    while r > 0:\n",
    "        word_idx += 1\n",
    "        r -= probs_vec[word_idx]\n",
    "    \n",
    "    return int_to_vocab[word_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate TV Script\n",
    "This will generate the TV script for you.  Set `gen_length` to the length of TV script you want to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "\n",
      "miss hoover: no! all four the magazines, use love more than you now. it's clearly even worth my cat now. but if you kept the end kirk and will take him to take a few dollars.\n",
      "lionel hutz:(pointed) in being on this school?\n",
      "lisa simpson:(gasps) they suffers out of the smart dream?\n",
      "ralph wiggum: to the simpson three back?\n",
      "lisa simpson: nothing.\n",
      "groundskeeper willie: more man in yer film.(to everyone) what are you ralph?\n",
      "seymour skinner: oh, you came, and it is... and to widow's children.\n",
      "lionel hutz: hmm. i'll be honest. so,?\n",
      "allison: how to switch-- you can finally have a rope,\" 'tis can still.\n",
      "chief wiggum:(making a little hard) by health four. hold a...\n",
      "bart simpson: daaad!\n",
      "\n",
      "(gas station: int. desolate systems man:(depressed) hey thank you, baby. this is hot here for times one?\n",
      "homer simpson: this bye?\n",
      "\n",
      "(indian puppy, suddenly nervous) lousy little school simpson!\n",
      "springfield:(heads) ooh, the spoon, doctor\" live.\"(squeaky teenage voice) it certainly is a surprise. curse the girls will stick into sex\" into it...\n",
      "fbi agent #2: pleasure enough.\n",
      "c. montgomery burns: no. what a long of our state's stringent, chocolate number center to kill the largest school for class.\n",
      "apu nahasapeemapetilon: ye saw it?\n",
      "grampa simpson: rabbi, the birthplace of employee is still my life.\n",
      "homer simpson:(intrigued) row. simpson, we'll do it ho-mer.\n",
      "\n",
      "(main cemetery: ext. elementary school - softball blimp bedroom - day)\n",
      "lisa simpson:\" here is careful and neddie. within losers. this isn't personal within\" x...(awkward phone) class!\n",
      "\n",
      "(springfield retirement castle: int. springfield streets)\n",
      "pilot: i'm gonna go about eye everyday days.\n",
      "grampa simpson: really?\n",
      "\n",
      "(springfield street: ext. springfield street - morning)\n",
      "jasper beardly: no,?\n",
      "homer simpson: and this is that colors as blue. he's a casino!\n",
      "\n",
      "(springfield elementary school: int. springfield town floor - establishing)\n",
      "groundskeeper willie:(screams)\n",
      "lisa simpson:(screams) ah, boys.\n",
      "chief wiggum: let's miss springfield showed for mayor, they own a popularity will my dog is that many springfield men to make a beauty.\n",
      "moe szyslak: ach, what he's gone.\n",
      "seymour skinner: sounds like there's smells like the superintendent sees that your energy.\n",
      "bart simpson:(covering) six of my third town. i would have coming to hit willie.\n",
      "jay: come on, sir, action!\n",
      "troy mcclure: it's real check, but he knows it, sir.\n",
      "dave jr. tongue has been hiccuping for all the kind of chocolate, but use the problem kennedy is on to be an idea is too\". didn't you do is for a sunday phone in one.\n",
      "homer simpson: forty int. montgomery burns:(car) hey, how nice at work,\" uncle force have a contract as a murder was a perfect country.\n",
      "waylon smithers: why, let's turn the attention career, paper for something-- ohh, we've got to tell anyone, or you're backwards.\n",
      "barney gumble: nonsense... to pick him up for all the case!(la voice.)\n",
      "fbi agent #2: no one.\n",
      "c. montgomery burns: oh no, look, and if yes it's my agent.\n",
      "homer simpson:(covering) slow down. you can't be forced to be the face.\n",
      "waylon smithers: we're shower.\"\" uh-huh.\n",
      "waylon smithers: what, still lost to the stranglehold of my radiation speech empty, regardless of personal and beauty trap, i'll be certainly i do bobo. he's rich. well, report to do.\n",
      "marge simpson: hey!\n",
      "c. montgomery burns: it's all this montgomery burns.(thinking) there'll be an orange.\n",
      "c. montgomery burns: next. we can't be sleeping?(then, simpson living room)\n",
      "c. montgomery burns: you never want to get a close for the american hypnotical association.\n",
      "waylon smithers: he's only right ahead, sir.\n",
      "c. montgomery burns: ewww! all! whoop-de-doo him.\n",
      "mona simpson: homer!\n",
      "\n",
      "(springfield nuclear power plant: int. power plant - parking lot - continuous)\n",
      "c. montgomery burns:(pained noise)\n",
      "marge simpson:(pleased) ooh, let's put home today. for the world. don't okay, homer. is still good, but i said you can fly off the bank to, mr montgomery clowns of the toilet?\n",
      "c. montgomery burns:(slightly scared) ah, smithers, go for blood, box. nick.(taunting) eh, those aren't all the real you.\n",
      "waylon smithers:(happy) no.\n",
      "kent brockman: the jars of urine?\n",
      "c. montgomery burns:(shrugs noise)\n",
      "c. montgomery burns:(kicks) well, no! except for terror home. uh, the man\n"
     ]
    }
   ],
   "source": [
    "gen_length = 1000\n",
    "\n",
    "prime_word = 'miss hoover:'\n",
    "\n",
    "\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "    print()\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = prime_word.split()\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        \n",
    "        pred_word = pick_word(probabilities[0][dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "\n",
    "        \n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "        \n",
    "    print(tv_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The TV Script is Nonsensical\n",
    "Our script doens't make an awful lot of sense, but the model has learned still learned some things! Character names are accurate and lines of dialogue come after the character's name followed by a colon. The model has also learned that a location should be specified at the  begining of a scene.\n",
    "\n",
    "I expect to get better results if I train a deeper network for more time on more data, which I plan to do when I get home from holidays (so that I'll have access to my desktop computer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
